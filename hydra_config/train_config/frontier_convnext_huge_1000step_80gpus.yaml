# Production ConvNeXt-V2 Huge 1000-step test configuration for Frontier
# Based on official ConvNeXt V2 Huge: 659M parameters, [352, 704, 1408, 2816] hidden_sizes

checkpoint:
  state_dict_type: sharded
  chkpt_saving_steps: 10      # Save every 10 steps for production tracking
  preempt_chkpt_saving_steps: 5  # Save preempt checkpoints every 5 steps
  directory: experiments/chkpts
  prefix: frontier-convnext-huge-1000step
  path_chkpt_prev: null  # Start from scratch
  offload_to_cpu: false
  rank0_only: false

dataset:
  drop_last_in_sampler: true
  drop_last_in_loader: true
  batch_size: 16  # Reduced from Hiera due to larger ConvNeXt model memory footprint
  num_workers: 2
  pin_memory: true
  prefetch_factor: 4
  path_train: pretrain/train.csv
  path_eval: pretrain/eval.csv
  debug: false
  cache_size: 10

  # Cross-rank shuffling configuration - Production settings
  enable_shuffling_train: true
  enable_shuffling_eval: true
  shuffle_seed_base: 42
  reshuffle_frequency: 100

  transforms:
    # Transform dimensions - Optimized for ConvNeXt
    H_pad: 1920
    W_pad: 1920
    Hv: 384         # ConvNeXt works well with 384x384 (divisible by patch_size=4)
    Wv: 384         # ConvNeXt works well with 384x384 (divisible by patch_size=4)
    sigma: 0.33
    num_crop: 1

    # Transform pipeline - Production settings
    set:
      pad: true
      polar_center_crop: true
      batch_sampler: false
      random_patch: false
      random_rotate: false
      random_shift: false

dist:
  backend: nccl
  uses_unique_world_seed: true
  dtype: bfloat16        # Production precision
  cpu_offload: null

logging:
  directory: experiments/logs
  prefix: frontier-convnext-huge-1000step
  level: INFO

loss:
  grad_accum_steps: 2    # Increased due to reduced batch size
  focal:
    alpha: [0.25, 0.75]
    gamma: 2.0

lr_scheduler:
  min_lr: 1.0e-07
  total_steps: 1000          # 1000 steps for the test run
  warmup_steps: 20           # 2% warmup (20/1000)
  scheduler_update_steps: 1

misc:
  max_eval_iter: 2          # Reasonable evaluation iterations
  max_eval_retry: 2
  sharding_stage: zero3      # FSDP for huge model memory efficiency
  compiles_model: false
  data_dump_on: false
  cpu_only: false
  peak_flops_per_sec: 312000000000000.0
  monitors_dynamics: false

model:
  from_scratch: true
  backbone:
    hf_config:
      # ConvNeXt V2 Huge official architecture - 659M parameters
      num_channels: 1
      patch_size: 4
      num_stages: 4
      hidden_sizes: [352, 704, 1408, 2816]    # Official ConvNeXt V2 Huge hidden sizes
      depths: [3, 3, 27, 3]                   # Deeper than base model for Huge variant
      hidden_act: "gelu"
      initializer_range: 0.02
      layer_norm_eps: !!float 1e-12
      drop_path_rate: 0.2                     # Higher dropout for huge model
      image_size: 384                         # Optimized input size for ConvNeXt
      out_features: ['stage1', 'stage2', 'stage3', 'stage4']
      out_indices: null
  
  bifpn:
    block:
      base_level: 4
      bn:
        eps: !!float 1.0e-05
        momentum: 0.1
      down_scale_factor: 0.5
      fusion:
        eps: !!float 1.0e-05
      num_features: 512                       # Increased for huge model
      num_levels: 4
      relu_inplace: false
      up_scale_factor: 2
    num_blocks: 3                             # More BiFPN blocks for huge model
  
  seg_head:
    base_scale_factor: 2
    num_classes: 2
    num_groups: 32
    out_channels: 512                         # Increased for huge model
    up_scale_factor:
    - 4
    - 8
    - 16
    - 32
    uses_learned_upsample: false

optim:
  grad_clip: 1.0
  lr: 5e-5              # Lower learning rate for huge model
  weight_decay: 0.05    # Standard weight decay for ConvNeXt
  beta1: 0.9
  beta2: 0.95           # ConvNeXt paper uses 0.95
  fused: false